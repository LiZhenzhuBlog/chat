<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>李珍珠</title>
    <div class="namemy">

    <h3>by:李珍珠</h3>
    <hr>
	</div>
    
    <style>
      svg.markmap {
        width: 100%;
        height: 90vh;
      }
  .image {
        display: flex; /* 使用Flexbox布局 */
        height: 6vh;

      flex-wrap: wrap;
        justify-content: space-between; /* 自动平均分布元素 */
    }
   .namemy {
        text-align: center;
        height: 2vh;
    }

  .image-container {
    width: 25%;; /* 设置图片容器的宽度 */
    
    position: relative; /* 设置相对定位以容纳图片和描述 */
  }

  .image-container img {
    width: 99%; /* 设置图片的宽度为容器的宽度 */
    height: 100%; /* 设置图片的高度为容器的高度 */
    object-fit: cover; /* 根据需要调整图片的适应方式 */
  }

  .image-container figcaption {
    position: absolute; /* 设置绝对定位以位于图片上方 */
    top: 0; /* 调整描述文本的位置 */
    width: 100%; /* 设置描述文本的宽度与容器一致 */
    background-color: rgba(0, 0, 0, 0.8); /* 调整背景颜色和透明度的值以满足您的需求 */
    color: white; /* 调整字体颜色以满足您的需求 */
    text-align: center;}

    </style>
    <script src="https://cdn.jsdelivr.net/npm/markmap-autoloader@0.14.4"></script>
  </head>
  <body>
    <div class="markmap">
    <script type="text/template">
	---
	markmap:
	  maxWidth: 1000
	  initialExpandLevel: 1
	---

    # TotalSegmentator：对CT图像中的104个解剖结构进行稳健分割
    ## 摘要：
    - 本研究旨在自动分割全身CT图像中的多个解剖结构。已有的分割算法存在以下问题：1. 使用困难（代码和数据不公开或难以使用）。2. 泛化能力有限（训练集通常只包含非常干净的图像，不能反映临床例行程序中的图像分布）。3. 该算法只能分割一个解剖结构，处理更多结构需要使用多个算法，增加了量。本研究发布了一个新的数据集和分割工具包来解决这三个问题：在1204个CT图像中分割了104个解剖结构（27个器官、59个骨骼、10个肌肉、8个血管），覆盖了使用场景。展示了一种改进的地面真实分割创建工作流程，将过程加速超过10倍。CT图像是从临床例行程序中随机采样的，代表了适用于临床应用的真实世界数据集。最后，通过个新数据集上训练，发布了一个名为TotalSegmentator的分割算法。该算法以预训练的Python pip软件包形式提供（pip install totalsegmentator），使用简单且对大多数CT图像效果良好。代码可在zenodo.6802613获取。

    ## 结果：
    - 1、总体结果
    		- 我们的两个模型（一个在1.5毫米分辨率上，另一个在3毫米上）在分割方面具有很好的准确性。图6展示了两个模型在所有类别上的均值结果。在使用1.5毫米切片厚度的CT进行训练的模型中，准确性很高：Dice和NSD都过0.96。对于使用3毫米切片厚度的模型，Dice分数下降至0.85，而NSD仍然超过0.96，因为较低分辨率引入的轻微不准确性仍在3毫米分辨率的范围内。因此可以得出结论，3毫米模型仍可提供正确的结果，但边界不够精确。对于许多应用来说，这是可以接受的。
    - 2、按类别的结果 
    		- 图5显示了各个类别的结果。我们将相似的类别（例如左右、所有肋骨、所有椎骨）进行分组以提高可读性。结果在0.87和1.0之间变化。
    - 3、与其他模型的比较
    		- 图7显示了我们的高分辨率模型（1.5毫米）与使用“Multi-Atlas Labeling Beyond the Cranial Vault Challenge”数据训练的nnU-Net之间的比较。我们的模型达到了更高的Dice分数和更高的NSD分数。
    - 4、 运行时间
    		- 表8显示了高分辨率（1.5毫米）和低分辨率（3毫米）模型在三个有不同尺寸的CT图像上的运行时间、RAM需求和GPU内存需求的概览情况。这些图像包括仅腹部的小尺寸图像，形状为512 x 512 x 280体素，和腹部的中等尺寸图像，形状为512 x 512 x 458体素，以及从头到膝盖的大尺寸图像，形状为512 x 512 x 824体素。运行时间是在一台装有NVidia GeForce RTX 3090 GPU的本地工作站上进行测量的。
    - 5、典型失败案例 
    		- 对于所有类别，我们的模型始终产生高度准确的结果。考虑到数据集的高多样性，这表明我们的模型具有鲁棒性。然而，在大约15%的受试者中会出现一些典型的失败案例，如图9所示。了解这些情况在使用受影响类别的分割时可能会有帮助。
    - 6、应用示例
    		- 作为我们的模型可以用于的一个例子，我们展示了一项衰老研究：不同解剖结构的密度（以霍恩斯菲尔德单位[HU]表示）以及体积在生命周期中的变化。为此，我们收集了4102个全身CT图像的数据集，这些图像是在多发伤的情况下进行的（有关年龄和性别分布，请参阅补充材料图12）。这些图像是从因事故而来医院就诊的患者那里获得的。通常情况下，只有一小部分解剖结构受到事故的损害。因此，由于所涵盖的案例数量较多言，该数据集代表了18至100岁患者的随机样本。作为示例，我们展示了三个解剖结构肝脏、心脏左心房和髋部（参见图10）。对肝脏，我们可以看到在50岁之前其体积逐渐增长，然后开始缩小。肝脏的密度沿着相反的轨迹变化。在60岁之前逐渐降低，然后再次。对于左心房，可以看到在整个生命周期中，它的体积和密度都在增加。对于髋部，我们可以看到其体积大约在60岁左右停止增长，并保持稳定。然而，骨密度在生命周期内不断下降。所有104个类别都可以进行同样的分析。
    

    ## 图片：
    - 图6. 高分辨率模型和低分辨率模型在所有别上的结果（Dice分数和标准化表面距离[NSD分数]）（柱状图显示95%置信区间。置信区间非常紧密，以至于被点完全覆盖）。
    - 图5. 高分辨率模型按类别的结果（相似的类别，例如肋骨或同一类别的左/右部分被分组在一起）。
    - 图7. 提出的模型（高分辨率模型）与公开可用的多器官分割模型的比较（柱状图显示95%置信区间）
    - 图9. 提出的模型的典型失败案例概述。用户应该注意到这些问题在大约15%的受试者中可能会出现。
    - 图10. 多发伤队列的4102名受试者整个生命周期内三个示例类别的平均密度（以HU为单位）和体积。可以看出，这两个指标在生命周期内发生了显著变化。（蓝线：二次模型的拟合）
    - 图1. TotalSegmentator可以对所有的104个解剖结构进行分割
    - 图2. 训练数据集的特征概述如下：1年龄分布，2性别分布，3数据采集地点分布，4.CT图像类型分布。
    - 图3. 显示了从真实标注的创建过程中的步骤示例：可以看到模型是如何从粗糙的真实标注分割中生成平滑的预测，并且如何纠正模型预测中的错误
    - 图4. 手动分割的主动学习方法首先，需要完全手动标记5个对象。然后训练一个模型，只需纠正模型的预测结果。随着更多数据的标记，模型将重新进行训练，减少需要手动纠正的数量。
    - 图8. 高分辨率模型（1.5mm）和低分辨率模型（3mm）在三个不同CT图像上的运行时间、RAM需求以及GPU内存需求的概述：一个仅包含腹部的小图像，大小为512x512x280体素；一个包含胸部和腹部的中等大小图像，大小为512x512x458体素；一个从头部到膝盖的大图像，大小为512x512x824体素。
    - 图11. 针对提出的高分辨率模型的所有104个类别的结果。（蓝色：Dice分数，橙色：NSD分数）。
    - 图12. 应用示例中使用的多发伤数据集的特征概述。1：年龄分布，2：性别分布。

    ## 结论：
    - 正如我们所展示的，我们的模型在一个非常多样化的数据集上产生了准确的分割结果，这个数据集直接来自于临床例行工作。我们希望许多研究人员能够发现我们的模型对他们的应用场景有所帮助。我们在github上公开提供了模型的代码（https://github.com/wasserth/TotalSegmentator），可以通过pip轻松安装：pip install totalsegmentator使用方法也很简单：TotalSegmentator -i ct.nii.gz -o seg_dir请注意，运行代码需要使用支持CUDA的NVidia GPU。对于没有GPU的用户，我们提供了一个在线工具，可以上传图像进行分割：www.totalsegmentator.com。我们还公开提供了包含1204个主体和104个地面真值分割的数据集：https://doi.org/10.5281/zenodo.6802613。在未来的工作中，我们计划进一步扩展类别的数量。目前，我们正在努力增加更多的血管分割功能。
    ## 方法：
    - 数据选择 
    		- 我们收集了以下数据集来训练我们的模型：在过去10年的时间跨度内，我们从我们的PACS系统中随机抽取了CT图像。只使用了对研究有一般同意的患者的图像。我们排除了双腿和手部的CT图像。我们还随机抽取了每个检查的CT系列。因此，该数据集包含具有不同序列（原始图像、动脉期、门静脉期、晚期、双能量）、有无造影剂、不同气球电压、不同层厚和分辨率以及不同内核（软组织内核、骨内核）的CT图像。如果人工专家标注者由于高度模糊性（例如，由病理学扭曲的结构）而不确定如何分割某些结构，则排除该检查（40个受试者）。最终的数据集包含1204张图像。图2显示了CT图像类型的分布、年龄分布和获取图像的地点分布（每个地点用字母匿名表示）。
    - 数据标注 
    		- 根据临床经验，我们确定了解剖结构的类别，这是我们模型用于分割的类别（请参见图1的图形概述和附加材料图11的完整列表）。完成一个类别的完整手动分割大约需要10分钟。基于在1204个受试者中手动分割104个类别所需的时间，一个人需要超过10年才能完成分割（假设该人每天工作8小时，每周工作5天）。这是不可行的。 为了加速这一过程，我们采用了以下方法，使其速度提高了几个数量级：
    		- 使用现有模型 
    			- 对于某些结构，如果已经有公开可用的现有模型，我们使用该模型创建了第一次分割，如果需要的话，再进行进一步手动调整精细化。考虑到我们数据集的高度变异性，通常需要进一步的精炼。附加材料显示了使用的模型的完整列表。这使我们对68个类别进行了第一次分割。对于剩余的36个类别，需要进行手动分割。分割的精炼和手动分割由获得认证的放射科医师进行监。
    		- 利用造影剂（CTA/CT分割）
    			- 在有和无造影剂的CT图像之间转移分割 为了能够在所有不同的CT系列中（包括没有造影剂的）对不同的心脏部位（左/右心、左/右心室、心肌、肺动脉）进行分割，采取了以下方法：我们有一个内部数据集，其中包含使用造影剂的CT图像上心脏部位的真实分割结果。对于每个有造影剂的图像，还有一张没有造影剂的原始CT图像。由于该图像来自同一个检查，因此与有造影剂的图像完全对齐。因此，我们将分割结果转移到没有造影剂的CT图像上，并在带有和不带有造影剂的图像上训练了一个U-Net模型（Ronneberger et al., 2015）。然后，我们使用这个模型在我们的数据集中预测心脏部位，从而在所有类型的CT序列中得到非常准确的心脏部位分割。
    		- 标注流程优化
    			- 为了加快手动分割的速度，我们使用了 Nora图像平台Anastasopoulos等，2017)。与传统工具如MITKWolf等，2005)相比，Nora提供了以下优势：首先，不需要逐个加载受试者，而是提供了所有受试者的列表。其次，在 Nora 中，不需要为每个图像单独创建和保存分割掩码，而是每当您在Nora中更改受试者时，正确的分割掩码会自动创建、命名和保存。第三，自动加载和视图（例如强度窗口）可以为每个项目单独配置。因此，我们在 Nora 中只需要17秒就能加载案例，创建一个分割掩码并保存分割掩码，而在MITK中，这个过程需要45秒。第四，通过进行粗糙的分割而不是像素完美的分割，可以实现更高的加速。这些粗糙的分割只是初步的，并不能满足最终结果的要求。然而，U-Net 在从粗糙的分割中学习良好的分割方面表现良好。粗糙分割的错误在受试者之间相对随机（没有系统性错误），因此U-Net学习忽略分割的随机部分，并保留与解剖结构相当一致的部分。图3显示了U-Net如何从粗糙的输入生成平滑的分割结果。第五，由于粗糙分割已经足够，我们使用了一支铅笔进行分割，一次可以同时穿过3个切片，从而提供了另一个加速。例如，对尿液膀胱进行详细分割需要10分钟，而粗糙分割只需要1分钟25秒，而U-Net也能从这些粗糙分割中产生准确的结果。总体而言，使用传统的分割方法，加载保存需要45秒，分割需要10分钟，而使用我们的方法，加载保存只需17秒，分割只需1分钟25秒。因此，这个过程可以加快7倍。
    		- 主动学习
    			- 为了进一步加速，我们采用了主动学习方法：在完成了前5个受试者的手动分割后，我们训练了一个U-Net，并只对其预测进行了修正。然后，在完成了20个、100个和1000个受试者的分割后，再次对U-Net进行重新训练。在每次迭代后，需要进行的手动修正就会减少。
    		- 质量控制的3D预览
    			- 通过使用3D渲染（使用Python的fury库）的掩模来检测错误，我们实现了另一个很大的加速。在进行了1或2次主动学习迭代后，大多数分割的预测结果不再需要进一步的改进。然而，加载所有掩模并滚动浏览所有切片来检查它们是否正确或需要修正花费了很多时间。通过生成一个包含所有104个掩模的3D渲染的PNG图像，可以极大地加速这项任务。我们在许多受试者上测试了这种方法，结果发现如果掩模存在错误，通常可以在3D渲染中发现（见图9）。此外，加载PNG图像是即时的，而加载104个掩模可能需要几分钟的时间。采用这种方法，可以在几秒钟内检查104个掩模是否存在错误，而传统方法可能需要几分钟的时间。仅基于3D渲染进行分割评估可能会忽略一些细小的错误。然而，对于主要解剖结构（与小型肺结节等相对比），在3D渲染中看不到的错误非常罕见。而且，如果逐个切片进行分割评估，由于专家评分人员在查看成千上万个切片时往往会变得疏忽，错误很容易被忽略。在几种情况下，我们在3D渲染中发现了被先前逐个切片评估所忽略的错误。
    		- 影像属性数据的处理
    			- 借助我们的方法，我们设定了大约8周的时间来完成分割工作，平均只需1个人工作。与完全手动分割需要10年的时间相比，这相当于加速了65倍。在许多情况下，采用我们的方法进行分割的质量也比完全手动分割更好：当专家进行手动分割时，必须逐个切片完成。因此，这些分割在切片之间往往存在轻微的不一致性，当从另一个方向或在3D渲染中查看时就可以看出来。此外，每当专家每天工作8小时时，他往往会感到厌倦，因此开始做得不那么精确。通过使用生成的分割结果，并仅纠正错误，我们的分割避免了这些问题。
    		- 伦理数据
    			- 脸部区域也被分割出来。最后，为了进行匿名化处理，所有的脸部都被模糊处理。此外，为了进一步保护隐私，每个受试者的年龄上添加了一个在-2到+2之间的随机数。
    - 模型
    		- 我们使用nnU-Net（Isensee等，2021; 2018; 2020）作为我们的模型。这是一个U-Net的实现，它根据数据集的特征自动配置所有超参数。这种方法已经被证明在提供最先进的结果的同时非常易于应用。我们对默认的nnU-Net设置进行了两个调整：我们移除了数据增强中的镜像操作，因为使用镜像操作时，模型无法正确区分左右骨骼。此外，考虑到数据集的规模较大，将训练的周期数从默认的1000增加到4000。对于低分辨率模型，由于一个模型中的类别数量较多（低分辨率模型有104个输出类别），训练时间更长，因此将训练周期进一步增加到8000。在不同的nnU-Net模型类型中，我们只使用了3D全分辨率模型。所有图像都被重新采样为1.5mm等各向同性分辨率。在这个分辨率上进行分割和模型训练。一般来说，nnU-Net可以同时训练预测所有104个类别。这需要大量的内存，因为nnU-Net需要同时存储所有类别的softmax预测结果，导致产生一个具有宽度x高度x深度x类别数的巨大图像。因此，104个类别被分为5个部分，每个部分包含21个类别，为每个部分训练一个nnU-Net模型。然而，对于大尺寸的图像，这仍然需要大量的RAM和GPU内存，而许多用户可能没有这样的设备。因此，我们还训练了一个将数据重新采样为3mm等各向同性分辨率的模型。考虑到较低的分辨率，可以在一个模型中训练所有104个类别，从而实现更快的运行时间。如果用户在我们的工具包中选择了“--fast”选项，则会使用这个3mm模型。在对数千个受试者进行大规模研究时，“--fast”选项可以帮助使如此多的受试者的处理变得可行。
    - 评估
    		- DICE和NSD分数评估
    			- 数据集中的1204个受试者随机分为1082个训练受试者（89.9%），57个验证受试者（4.7%）和65个用于最终测试的受试者（5.4%）。所有结果都是基于测试集报告的。作为评估指标，我们使用了Dice分数和标准化表面距离（NSD）（使用来自https://github.com/deepmind/surface-distance的“计算公差下的表面Dice”函数）。对于许多应用（例如某个器官的定位），只要整体分割是正确的，就不需要精确地分割器官边界。因此，表面距离是一个很好的度量标准。我们将阈值设置为3毫米。这意味着如果分割表面的所有区域与地面真实分割表面的距离在3毫米以内，则被视为正确。与Dice分数一样，NSD分数的范围在0到1之间，1是最好的分数。此外，Dice分数对大类别有偏好：内部体素通常比边界体素更容易预测，但在Dice分数中它们被等权重考虑。在大类别中，内部体素与边界体素的比例通常较高，因此Dice分数往往较好。相比之下，具有3毫米高阈值的标准化表面距离对小类别有偏好：对于像髂动脉这样的细小结构，几乎整个结构都在3毫米的阈值范围内。因此，分割可能会丢失类别的某些部分，但仍会被视为正确，因为它在3毫米的阈值范围内。因此，同时考虑这两个指标对于正确评估是重要的。对于每个受试者的每个类别，计算相应的度量指标，然后在所有类别和受试者之间取平均值（微平均）。
    		- 对比其他模型评估
    			- 作为额外的评估，我们将我们的模型与在“Multi-Atlas Labeling Beyond the Cranial Vault Challenge”数据集上训练的nnU-Net进行了比较（https://doi.org/10.7303/syn3193805，nnU-Net任务17）。我们使用这个模型对我们的测试集进行预测，并将结果与我们的模型进行了比较。该数据集仅提供了13个类别的标签。因此，比较仅限于这些类别。
    ## 讨论：
    
    ## 简介：
    - 不断增加的放射影像数量推动了对基于求。自动分析放射影像的一个重要组成部分是主要解剖结构的分割。在特定解剖结构的分割 Radhakrishnan，2015)、肺 (Hofmanninger 等人，2020) 等等。还有关于在一个数据集和模型中分割多个解剖结构的研究（例如多个器官 (Kavur 等人，2021)、(Okada 等人，2012)、(Rister 等人，2020)、(Norajitra 和 Maier-Hein，2017)、(Chen 等人，2021)) 或多个椎骨 (Sekuboyina 等人，2021)。
    - 然而，所有这些模型只涵盖了一小部分相关的解剖结构，并且在不代表典型临床人群的数据集上进行训练。
    - 据我们所知，这是第一个试图分割超过100个类别的工作。最接近的研究来自 Chen 等人（Chen et al., 2021），他们分割了50个类别。然而，他们仍然缺少许多类别，数据集和模型都不公开可用（模型只能通过上传数据到Web界面进行访问），而且数据集的多样性较差（大部分训练数据来自同一台扫描仪，并使用相同的CT序列）。
    - 通过我们的方法，我们解决了这三个问题：我们的模型可以分割广泛的解剖结构。我们的模型在临床数据上具有强大的鲁棒性，因为它是在大规模临床数据集上进行训练的。模型和训练数据都是公开可用的，并且非常易于使用。
    
    ## GPT检索的未知知识
    - Dice分数是一种广泛用于评估图像分割准确性的指标。它衡量了预测分割结果与真实分割结果之间的相似程度。Dice分数的计算基于预测分割结果和真实分割结果之间的重叠区域。它通过计算两者之间的重叠体素数量的两倍除以它们各自的总体素数量的总和来获得。即，Dice分数的计算公式为：Dice = (2 * |预测分割结果 ∩ 真实分割结果|) / (|预测分割结果| + |真实分割结果|)其中，|A|表示集合A的元素数量。Dice分数的取值范围在0到1之间，其中1表示完美的一致性，而0表示没有重叠区域。较高的Dice分数表示预测分割结果与真实分割结果的一致性较好。因此，较高的Dice分数表示更准确的分割结果。

      </script>
	</div>


<div class="image">
	<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>
	<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>
		<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>
		<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://img2.duote.com/duoteimg/dtnew_softup_img/202201/20220108013310_19007.jpg" alt="图片22描述">
	</div>
		<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>
		<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>
		<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>
		<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>
		<div class="image-container">
			<figcaption>图片描述</figcaption>
			<img src="https://v2.img.360kuai.com/video/360_202_/t0113ee287463f4cdc3.jpg" alt="图片22描述">
	</div>

</div>

  </body>
</html>